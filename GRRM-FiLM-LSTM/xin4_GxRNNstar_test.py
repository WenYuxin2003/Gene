# -*- coding: utf-8 -*-
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from xin4_GxRNNstar import GxRNN  # è‹¥ä½ ä¿å­˜æˆ GxRNN_star.pyï¼Œè¯·æ”¹æˆ from GxRNN_star import GxRNN
from utils import vocabulary
from types import SimpleNamespace
from tqdm import tqdm
import os

# ==============================
# 1ï¸âƒ£ å‚æ•°é…ç½®
# ==============================
args = SimpleNamespace(
    gene_expression_file="datasets/LINCS_reordered/",
    cell_name="mcf7_reordered",
    gene_num=978,
    emb_size=128,
    hidden_size=512,
    num_layers=2,
    dropout=0.2,
)

BATCH_SIZE = 32
EPOCHS = 10
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")

# ==============================
# 2ï¸âƒ£ è½½å…¥æ•°æ®
# ==============================
path = os.path.join(args.gene_expression_file, f"{args.cell_name}.csv")
if not os.path.exists(path):
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {path}")

data = pd.read_csv(path, header=None)
data = data.iloc[:1000]  # ä¸ºå¿«é€Ÿæµ‹è¯•åªå–å‰ 1000 æ¡ï¼Œå¯æ”¹å¤§
print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {data.shape[0]} æ¡æ ·æœ¬, {data.shape[1]} åˆ—")

smiles = data.iloc[:, 1].values
genes = torch.tensor(data.iloc[:, 2:].values, dtype=torch.float32)

# tokenizer
tokenizer = vocabulary(args)

def encode_smiles(s):
    return torch.tensor(tokenizer.encode(s), dtype=torch.long)

encoded = [encode_smiles(s) for s in smiles]
max_len = max(len(s) for s in encoded)
padded = torch.zeros(len(encoded), max_len, dtype=torch.long)
for i, s in enumerate(encoded):
    padded[i, :len(s)] = s

dataset = list(zip(padded, genes))
loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# ==============================
# 3ï¸âƒ£ åˆå§‹åŒ– STAR-GxRNN æ¨¡å‹
# ==============================
model = GxRNN(
    tokenizer=tokenizer,
    emb_size=args.emb_size,
    hidden_size=args.hidden_size,
    gene_latent_size=args.gene_num,
    num_layers=args.num_layers,
    dropout=args.dropout,
    star_core_dim=64,       # STAR çš„æ ¸å¿ƒç»´åº¦
    gene_feature_dim=128,   # åŸºå› èåˆåçš„ç‰¹å¾ç»´åº¦
).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = torch.nn.NLLLoss(ignore_index=tokenizer.char_to_int[tokenizer.pad])

# ==============================
# 4ï¸âƒ£ è®­ç»ƒå¾ªç¯
# ==============================
loss_history = []
model.train()
for epoch in range(EPOCHS):
    total_loss = 0
    for smiles_batch, genes_batch in tqdm(loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
        smiles_batch, genes_batch = smiles_batch.to(device), genes_batch.to(device)
        optimizer.zero_grad()
        outputs = model(smiles_batch, genes_batch)
        loss = criterion(outputs, smiles_batch.view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(loader)
    loss_history.append(avg_loss)
    print(f"Epoch {epoch+1}/{EPOCHS} - å¹³å‡loss: {avg_loss:.4f}")

# ==============================
# 5ï¸âƒ£ ç»˜åˆ¶ loss æ›²çº¿
# ==============================
os.makedirs("results", exist_ok=True)
plt.figure(figsize=(6, 4))
plt.plot(range(1, len(loss_history) + 1), loss_history, marker="o")
plt.title("Training Loss over Epochs (STAR-GxRNN)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.tight_layout()
plt.savefig("results/star_gxrnn_loss_curve.png", dpi=150)
plt.show()
print("ğŸ“‰ å·²ä¿å­˜ loss æ›²çº¿åˆ° results/star_gxrnn_loss_curve.png")

# ==============================
# 6ï¸âƒ£ åˆ†å­ç”Ÿæˆæµ‹è¯•
# ==============================
model.eval()
with torch.no_grad():
    latent_vectors = genes[:5].to(device)
    generated = model.sample(max_len=80, latent_vectors=latent_vectors)

# decode back to smiles
def decode_smiles(tensor):
    return tokenizer.decode(tensor.tolist())

print("\nğŸ§ª ç”Ÿæˆçš„å‰ 5 æ¡åˆ†å­ SMILESï¼š")
for i, gen in enumerate(generated):
    smiles_str = decode_smiles(gen)
    print(f"[{i+1}] {smiles_str}")

print("\nâœ… STAR-GxRNN æµ‹è¯•å®Œæˆï¼")
