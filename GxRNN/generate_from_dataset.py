# -*- coding: utf-8 -*-
"""
ä½¿ç”¨å·²è®­ç»ƒå¥½çš„ GxRNN æ¨¡å‹ (.pkl) é‡æ–°ç”Ÿæˆè®­ç»ƒé›†ä¸éªŒè¯é›†åˆ†å­
å¹¶å¯¼å‡ºåŸå§‹åˆ†å­ (SMILES) ä»¥ä¾¿å¯¹æ¯”
Author: æ•”é‘«
"""

import torch
import pandas as pd
from rdkit import Chem
from utils import vocabulary, get_device
from xin4_GxRNNstar import GxRNN
from train_gxrnn import load_smiles_data

# ==============================
# å‚æ•°é…ç½®ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒå®Œå…¨ä¸€è‡´ï¼‰
# ==============================
class Args:
    gene_expression_file = "datasets/LINCS/"   # æ•°æ®è·¯å¾„
    cell_name = "mcf7"                         # æ•°æ®é›†å
    gene_num = 978
    gene_batch_size = 64
    train_rate = 0.9
    emb_size = 128
    hidden_size = 1024                         # ä¸è®­ç»ƒæ—¶ç›¸åŒ
    num_layers = 3
    smiles_dropout = 0.3
    max_len = 100
    saved_gxrnn = "results/saved_gxrnn.pkl_450.pkl"  # æ¨¡å‹è·¯å¾„
    variant=False

args = Args()

# ==============================
# åŠ è½½è¯å…¸ä¸æ•°æ®é›†
# ==============================
print("ğŸ“˜ Loading dataset & tokenizer ...")
tokenizer = vocabulary(args)
train_loader, valid_loader = load_smiles_data(tokenizer, args)

# ==============================
# åŠ è½½æ¨¡å‹
# ==============================
print(f"ğŸ“¦ Loading trained model from {args.saved_gxrnn} ...")
model = GxRNN(
    tokenizer,
    emb_size=args.emb_size,
    hidden_size=args.hidden_size,
    gene_latent_size=args.gene_num,
    num_layers=args.num_layers,
    dropout=args.smiles_dropout
).to(get_device())

model.load_model(args.saved_gxrnn)
model.eval()

# ==============================
# è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆ + ç­›é€‰ + ä¿å­˜åŸå§‹åˆ†å­
# ==============================
def generate_and_save(loader, tag):
    print(f"\nğŸš€ Generating molecules for {tag} set ...")
    generated, original = [], []

    for _, (smiles, genes) in enumerate(loader):
        smiles, genes = smiles.to(get_device()), genes.to(get_device())
        dec_sampled_char = model.sample(args.max_len, genes)

        # è§£ç ç”Ÿæˆåˆ†å­
        gen_smiles = [
            "".join(tokenizer.decode(dec_sampled_char[i].squeeze().detach().cpu().numpy())).strip("^$ ")
            for i in range(dec_sampled_char.size(0))
        ]

        # è§£ç çœŸå®åˆ†å­
        true_smiles = [
            "".join(tokenizer.decode(smiles[i].squeeze().detach().cpu().numpy())).strip("^$ ")
            for i in range(smiles.size(0))
        ]

        # è¿‡æ»¤ç”Ÿæˆåˆ†å­ï¼ˆåªä¿ç•™åˆæ³•çš„ï¼‰
        for g_smi, t_smi in zip(gen_smiles, true_smiles):
            mol = Chem.MolFromSmiles(g_smi)
            if mol is not None and mol.GetNumAtoms() > 1:
                generated.append(Chem.MolToSmiles(mol))
                original.append(t_smi)

    # ä¿å­˜ç”Ÿæˆç»“æœ
    gen_df = pd.DataFrame({"SMILES": generated})
    gen_path = f"results/generated_{tag}_from_pkl.csv"
    gen_df.to_csv(gen_path, index=False)

    # ä¿å­˜åŸå§‹çœŸå®åˆ†å­
    orig_df = pd.DataFrame({"SMILES": original})
    orig_path = f"results/original_{tag}_smiles.csv"
    orig_df.to_csv(orig_path, index=False)

    print(f"âœ… {tag} é›†ç”Ÿæˆå®Œæˆ: {len(generated)} ä¸ªåˆæ³•åˆ†å­")
    print(f"ğŸ“ ç”Ÿæˆåˆ†å­ä¿å­˜åˆ°: {gen_path}")
    print(f"ğŸ“ åŸå§‹åˆ†å­ä¿å­˜åˆ°: {orig_path}")
    print("ç¤ºä¾‹å‰5ä¸ªç”Ÿæˆåˆ†å­:\n", gen_df.head().to_string(index=False))


# ==============================
# æ‰§è¡Œç”Ÿæˆ
# ==============================
generate_and_save(train_loader, "train")
generate_and_save(valid_loader, "valid")

print("\nğŸ‰ è®­ç»ƒé›†ä¸éªŒè¯é›†ç”Ÿæˆä¸åŸå§‹åˆ†å­å¯¼å‡ºå®Œæˆã€‚")
